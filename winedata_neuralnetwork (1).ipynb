{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "winedata-neuralnetwork.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhqrBITYaqQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYQMQNKsa8q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df = pd.read_csv('wine.csv')\n",
        "# print(df)\n",
        "a = pd.get_dummies(df['Wine'])\n",
        "df = pd.concat([df,a],axis=1)\n",
        "X = df.drop([1, 2,3,'Wine'], axis = 1)\n",
        "y = df[[1,2,3]].values\n",
        "X_train, X_test, Y_train,Y_test = train_test_split(X, y, test_size=0.20,)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxGUW1hVbw_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(model,a0):\n",
        "    \n",
        "    # Load parameters from model\n",
        "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
        "    # Do the first Linear step \n",
        "    # Z1 is the input layer x times the dot product of the weights + bias b\n",
        "    z1 = a0.dot(W1) + b1\n",
        "    \n",
        "    # Put it through the first activation function\n",
        "    a1 = np.tanh(z1)\n",
        "    \n",
        "    # Second linear step\n",
        "    z2 = a1.dot(W2) + b2\n",
        "    \n",
        "    # Second activation function\n",
        "    a2 = np.tanh(z2)\n",
        "    \n",
        "    #Third linear step\n",
        "    z3 = a2.dot(W3) + b3\n",
        "    \n",
        "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
        "    a3 = softmax(z3)\n",
        "    \n",
        "    #Store all results in these values\n",
        "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
        "    return cache"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdBzknOqb1vQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(z):\n",
        "    #Calculate exponent term first\n",
        "    exp_scores = np.exp(z)\n",
        "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o19mhCcwb5Qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_prop(model,cache,y):\n",
        "\n",
        "    # Load parameters from model\n",
        "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
        "    \n",
        "    # Load forward propagation results\n",
        "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
        "    \n",
        "    # Get number of samples\n",
        "    m = y.shape[0]\n",
        "    \n",
        "    # Calculate loss derivative with respect to output\n",
        "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
        "\n",
        "    # Calculate loss derivative with respect to second layer weights\n",
        "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
        "    \n",
        "    # Calculate loss derivative with respect to second layer bias\n",
        "    db3 = 1/m*np.sum(dz3, axis=0)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer\n",
        "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer weights\n",
        "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer bias\n",
        "    db2 = 1/m*np.sum(dz2, axis=0)\n",
        "    \n",
        "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
        "    \n",
        "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
        "    \n",
        "    db1 = 1/m*np.sum(dz1,axis=0)\n",
        "    \n",
        "    # Store gradients\n",
        "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
        "    return grads"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JUhtyCtb9dA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cddf71d8-d30a-4d9f-c029-bc67dbd954af"
      },
      "source": [
        "def softmax_loss(y,y_hat):\n",
        "    # Clipping value\n",
        "    minval = 0.000000000001\n",
        "    # Number of samples\n",
        "    m = y.shape[0]\n",
        "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
        "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
        "    return loss\n",
        "\n",
        "def loss_derivative(y,y_hat):\n",
        "    return (y_hat-y)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return (1 - np.power(x, 2))\n",
        "\n",
        "\n",
        "#TRAINING PHASE\n",
        "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
        "    # First layer weights\n",
        "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
        "    \n",
        "    # First layer bias\n",
        "    b1 = np.zeros((1, nn_hdim))\n",
        "    \n",
        "    # Second layer weights\n",
        "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
        "    \n",
        "    # Second layer bias\n",
        "    b2 = np.zeros((1, nn_hdim))\n",
        "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
        "    b3 = np.zeros((1,nn_output_dim))\n",
        "    \n",
        "     # Package and return model\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
        "    return model\n",
        "\n",
        "def update_parameters(model,grads,learning_rate):\n",
        "    # Load parameters\n",
        "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
        "    \n",
        "    # Update parameters\n",
        "    W1 -= learning_rate * grads['dW1']\n",
        "    b1 -= learning_rate * grads['db1']\n",
        "    W2 -= learning_rate * grads['dW2']\n",
        "    b2 -= learning_rate * grads['db2']\n",
        "    W3 -= learning_rate * grads['dW3']\n",
        "    b3 -= learning_rate * grads['db3']\n",
        "    \n",
        "    # Store and return parameters\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
        "    return model\n",
        "\n",
        "def predict(model, x):\n",
        "    # Do forward pass\n",
        "    c = forward_prop(model,x)\n",
        "    #get y_hat\n",
        "    y_hat = np.argmax(c['a3'], axis=1)\n",
        "    return y_hat\n",
        "\n",
        "def calc_accuracy(model,x,y):\n",
        "    # Get total number of examples\n",
        "    m = y.shape[0]\n",
        "    # Do a prediction with the model\n",
        "    pred = predict(model,x)\n",
        "    # Ensure prediction and truth vector y have the same shape\n",
        "    pred = pred.reshape(y.shape)\n",
        "    # Calculate the number of wrong examples\n",
        "    error = np.sum(np.abs(pred-y))\n",
        "    # Calculate accuracy\n",
        "    return (m - error)/m * 100\n",
        "losses = []\n",
        "\n",
        "def train(model,X_,y_,learning_rate, epochs, print_loss=False):\n",
        "    # Gradient descent. Loop over epochs\n",
        "    for i in range(0, epochs):\n",
        "\n",
        "        # Forward propagation\n",
        "        cache = forward_prop(model,X_)\n",
        "\n",
        "        # Backpropagation\n",
        "        grads = backward_prop(model,cache,y_)\n",
        "        \n",
        "        # Gradient descent parameter update\n",
        "        # Assign new parameters to the model\n",
        "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
        "    \n",
        "        # Pring loss & accuracy every 100 iterations\n",
        "        if print_loss and i % 100 == 0:\n",
        "            a3 = cache['a3']\n",
        "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
        "            y_hat = predict(model,X_)\n",
        "            y_true = y_.argmax(axis=1)\n",
        "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
        "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
        "    return model\n",
        "\n",
        "# This is what is returned at the end\n",
        "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
        "model = train(model,X_train,Y_train,learning_rate=0.07,epochs=4500,print_loss=True)\n",
        "plt.plot(losses)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0 : 1.5705845599375023\n",
            "Accuracy after iteration 0 : 21.12676056338028 %\n",
            "Loss after iteration 100 : 0.49633665194774906\n",
            "Accuracy after iteration 100 : 80.28169014084507 %\n",
            "Loss after iteration 200 : 0.44325837418740593\n",
            "Accuracy after iteration 200 : 80.98591549295774 %\n",
            "Loss after iteration 300 : 0.4086611626043213\n",
            "Accuracy after iteration 300 : 83.09859154929578 %\n",
            "Loss after iteration 400 : 0.38464353693784264\n",
            "Accuracy after iteration 400 : 83.80281690140845 %\n",
            "Loss after iteration 500 : 0.34984594589131035\n",
            "Accuracy after iteration 500 : 86.61971830985915 %\n",
            "Loss after iteration 600 : 0.33124201235963413\n",
            "Accuracy after iteration 600 : 88.02816901408451 %\n",
            "Loss after iteration 700 : 0.3201785470358225\n",
            "Accuracy after iteration 700 : 88.02816901408451 %\n",
            "Loss after iteration 800 : 0.3129590913077653\n",
            "Accuracy after iteration 800 : 88.73239436619718 %\n",
            "Loss after iteration 900 : 0.30681894682899485\n",
            "Accuracy after iteration 900 : 88.73239436619718 %\n",
            "Loss after iteration 1000 : 0.3010169291663438\n",
            "Accuracy after iteration 1000 : 88.73239436619718 %\n",
            "Loss after iteration 1100 : 0.28600623737753406\n",
            "Accuracy after iteration 1100 : 89.43661971830986 %\n",
            "Loss after iteration 1200 : 0.26790751104565946\n",
            "Accuracy after iteration 1200 : 90.84507042253522 %\n",
            "Loss after iteration 1300 : 0.2505938104437247\n",
            "Accuracy after iteration 1300 : 91.54929577464789 %\n",
            "Loss after iteration 1400 : 0.24390211594787578\n",
            "Accuracy after iteration 1400 : 91.54929577464789 %\n",
            "Loss after iteration 1500 : 0.2388754702722446\n",
            "Accuracy after iteration 1500 : 91.54929577464789 %\n",
            "Loss after iteration 1600 : 0.23048296213192737\n",
            "Accuracy after iteration 1600 : 92.25352112676056 %\n",
            "Loss after iteration 1700 : 0.22542657404438937\n",
            "Accuracy after iteration 1700 : 92.25352112676056 %\n",
            "Loss after iteration 1800 : 0.22161144689100967\n",
            "Accuracy after iteration 1800 : 92.25352112676056 %\n",
            "Loss after iteration 1900 : 0.21206587946415112\n",
            "Accuracy after iteration 1900 : 92.95774647887323 %\n",
            "Loss after iteration 2000 : 0.20030929342041945\n",
            "Accuracy after iteration 2000 : 92.95774647887323 %\n",
            "Loss after iteration 2100 : 0.17036835242511705\n",
            "Accuracy after iteration 2100 : 93.66197183098592 %\n",
            "Loss after iteration 2200 : 0.1656013181850814\n",
            "Accuracy after iteration 2200 : 94.36619718309859 %\n",
            "Loss after iteration 2300 : 0.16188710561891456\n",
            "Accuracy after iteration 2300 : 94.36619718309859 %\n",
            "Loss after iteration 2400 : 0.157379431737365\n",
            "Accuracy after iteration 2400 : 94.36619718309859 %\n",
            "Loss after iteration 2500 : 0.14670194745368365\n",
            "Accuracy after iteration 2500 : 95.07042253521126 %\n",
            "Loss after iteration 2600 : 0.1411296863403175\n",
            "Accuracy after iteration 2600 : 95.07042253521126 %\n",
            "Loss after iteration 2700 : 0.13569407194054\n",
            "Accuracy after iteration 2700 : 95.07042253521126 %\n",
            "Loss after iteration 2800 : 0.128083834178591\n",
            "Accuracy after iteration 2800 : 95.07042253521126 %\n",
            "Loss after iteration 2900 : 0.12022106108768928\n",
            "Accuracy after iteration 2900 : 95.07042253521126 %\n",
            "Loss after iteration 3000 : 0.11199477296810387\n",
            "Accuracy after iteration 3000 : 95.07042253521126 %\n",
            "Loss after iteration 3100 : 0.09370870786733944\n",
            "Accuracy after iteration 3100 : 96.47887323943662 %\n",
            "Loss after iteration 3200 : 0.09035816853371532\n",
            "Accuracy after iteration 3200 : 97.1830985915493 %\n",
            "Loss after iteration 3300 : 0.08807580002483845\n",
            "Accuracy after iteration 3300 : 97.1830985915493 %\n",
            "Loss after iteration 3400 : 0.08568112730509252\n",
            "Accuracy after iteration 3400 : 97.1830985915493 %\n",
            "Loss after iteration 3500 : 0.0828213733697971\n",
            "Accuracy after iteration 3500 : 97.1830985915493 %\n",
            "Loss after iteration 3600 : 0.07924637522828705\n",
            "Accuracy after iteration 3600 : 97.1830985915493 %\n",
            "Loss after iteration 3700 : 0.07536234668752671\n",
            "Accuracy after iteration 3700 : 97.88732394366197 %\n",
            "Loss after iteration 3800 : 0.07127242867286972\n",
            "Accuracy after iteration 3800 : 97.88732394366197 %\n",
            "Loss after iteration 3900 : 0.06900505937715852\n",
            "Accuracy after iteration 3900 : 97.88732394366197 %\n",
            "Loss after iteration 4000 : 0.06802511709618116\n",
            "Accuracy after iteration 4000 : 97.88732394366197 %\n",
            "Loss after iteration 4100 : 0.067331900250271\n",
            "Accuracy after iteration 4100 : 97.88732394366197 %\n",
            "Loss after iteration 4200 : 0.06673356626735942\n",
            "Accuracy after iteration 4200 : 97.88732394366197 %\n",
            "Loss after iteration 4300 : 0.0661586750453021\n",
            "Accuracy after iteration 4300 : 97.88732394366197 %\n",
            "Loss after iteration 4400 : 0.06552122874933815\n",
            "Accuracy after iteration 4400 : 97.88732394366197 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2d1225c940>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZiklEQVR4nO3de3CU933v8fdXEgIhLkIXZIzAXI3tJAY7Mubiti7k4lwmJq3rJm0SkpJDzpxM4jTpaezMmUnvY09z6uR00rg0dkzT1DZ17eLxOU3jEttpIMYWNr7ENyQMBgJIiIvNSuxqV9/zxz4LuixY2mfRss/zec0w2n329uUZ68PPv99vn6+5OyIiEi0VpS5ARESKT+EuIhJBCncRkQhSuIuIRJDCXUQkgqpKXQBAY2Ojz5kzp9RliIiUlR07dhxx96Z8j10Q4T5nzhza2tpKXYaISFkxs71ne+wdp2XM7B4z6zSzlwYcqzezx8xsV/BzWnDczOz/mFm7mb1gZlcX568gIiKjMZI593uBG4YcuxXY4u4LgS3BfYAPAQuDP+uB7xWnTBERGY13DHd3/xlwdMjhG4GNwe2NwJoBx//Rs54C6sxsRrGKFRGRkSl0t0yzux8Mbh8CmoPbM4F9A563Pzg2jJmtN7M2M2vr6uoqsAwREckn9FZIz16cZtQXqHH3De7e6u6tTU15F3tFRKRAhYb74dx0S/CzMzh+AJg14HktwTERERlDhYb7I8Da4PZaYPOA458Jds0sA04MmL4REZEx8o773M3sPuB6oNHM9gPfBG4HNpnZOmAvcHPw9P8HfBhoB3qAz52HmkUk5rpPJtnW0c2uzpNQ5pctX315M4tn1RX9fd8x3N39k2d5aHWe5zrwxbBFiYgMdDKZ5uk3utna3s3W9iO8eujt04+ZlbCwIpg+ZUJpwl1ExN3p6DrJ1vZu2vYeozeVGbPPPppI8sL+E6T7nfFVFbTOmcb//OAiVi5o5N0XT6GqUpfIykfhLiJ5/ep4L1vbj7Cto5ttHUc4/FYSgIunTqBuYvWY1VE7vpIv/MY8Vs5v5OpLpjFhXOWYfXY5U7iLyGn9/c6/v3SIv/3prtNTH/W11ayY38DKBY2snN/I7IaJJa5SRkLhLiK4O0+83sW3/uM1fvmrt7i0eRL/6yOXs2J+I5ddNJmKijKf2I4hhbtIzD39xlH++j9e5Zk9x5hVX8Odv7uYjy2eSaUCvawp3EXGUOfbp2jvPFnqMgA41Zdh47a9PPl6F9Mnj+fP17yb322dRXWVFiijQOEuch69daqP7buPBguTR3j98IUR7Dl1E8dx24cu4zPL51BTrYXKKFG4i+SR6XdePHCC5/cdJ90/+i/JHE1kv2Tzwv4TZPqdCeMquGZOPb91dQvvmTn1gpjyMOCKi6cwecK4Upci54HCXYTsgmJ750m2th9ha0c3T+3u5u1T6YLfr7LCWNwylf9x/XxWLmjkqtl1jK/SyFjGjsJdYutAbh93sJe78+3sPu5Z9TV85D0zWLGgkaVz6guarhhfVaH92FJSCneJjWOJFL/Ynf36+tb2I+zp7gGgobaaFQsaWRns5Z5Vr33cUv4U7lLWTvVl2LH3GFvbj/DMnqOcTOb/WnwyneGNIwncoba6kmvnNfCpZZewckEji5q1j1uiR+EuZSW30JnbffLMnmOk0v1UVhhXtkylZVpN3tdVGKxZMpOVCxq4sqWOcboeiUScwl3GzNnmuAt12UWT+dS1l3DdwgaWzm1g0nj95yySo98GeUeJZJq9wfz0aDjO3u4efh4Eem6Ou3HSeFbMb2BOYy2FTIbMnz6JFfMbaJw0voBXi8SDwl2GSaX72bnv+OlQ3lngXu+cSeOrWDavns8sn8PKBY1c2jwJK/eLcItc4EKFu5ndAvw3st+H+Ad3/7aZ1QMPAHOAPcDN7n4sZJ1ynnW+fYp/e+4AW9u7efqNo/T2ZTCD98ycyud/bV7BX7yZPmU8V86cqmtui4yxgsPdzN5NNtiXAingx2b2KLAe2OLut5vZrcCtwNeLUawU34mePu76WQc/2PoGp/r6md9Uy++0trBifiPL5zUwdaK+vShSjsKM3C8Htrt7D4CZPQn8FnAj2Z6rABuBJ1C4X3ASyTT3btvDXU92cDKZ5sbFF/Pl1QuZ1zSp1KWJSBGECfeXgL80swagl2xj7Dag2d0PBs85BDSHK1GKKZnO8M/b3+S7j7dz5GSK913ezNc+cCmXz5hS6tJEpIgKDnd3f8XM7gB+AiSAnUBmyHPczPKuxJnZerJTOMyePbvQMmQEelJpnn7jKNs6uvm/LxzkwPFels9rYMNnFnH17GmlLk9EzoNQC6rufjdwN4CZ/RWwHzhsZjPc/aCZzQA6z/LaDcAGgNbW1sK3YsgwfZl+nt93PNspvuMIz715jL6MU11ZwTVzp3HHb1/JygUN2rEiEmFhd8tMd/dOM5tNdr59GTAXWAvcHvzcHLrKmHDP7gvf2nGEbe3dtO09yqm+/lG/z6m+DMl0P2bw7oun8gfXzWXl/EauKfAiWCJSfsLuc//XYM69D/iiux83s9uBTWa2DtgL3By2yHJ3MpnmRG9f3sfSmeye8uzFrLo5cLwXgIumTGD5vIaCusxXV1Vw9ew6lhX4ehEpf2GnZX4tz7FuYHWY942KY4kUdz3Zwb3b9pBMn3sEPmVCFcvnN/CF35jHivmNzG+q1bSJiBRM31A9D04m09zz8zf4h5/t5mQqzZolM1k+ryH/ky17jZR3XXxhdOcRkWhQuBfRqb4M//TUXv7uiQ6OJlJ88F3NfO0Di7i0eXKpSxORmFG4F8nmnQe4/d9f5eCJU1y3oJE/+uAilsyqK3VZIhJTCvciuO/pN7ntoRdZ3DKV//07i1mxoLHUJYlIzCncQ9q88wDfePhFrl/UxIZPt1JdpQtkiUjpKYlC+PFLh/jqpue5dm49d33qvQp2EblgKI0K9MRrnXzpvme5smUq3197jTrdi8gFReFegKd2d/OFH+5g4fTJ3PvZpWrvJiIXHIX7KO3cd5x19z7DrPqJ/HDdUl3vXEQuSBpyjpC7s3PfcT77g2domDSeH33+WhrUw1NELlAK93PofOsUWzuy13z5RUf2ui8zpk7gR5+/luYpE0pdnojIWSnch3jpwAke3LGfre1H2NV5EoC6ieNYPq+B/379fG5410U0TdaIXUQubAr3gLvzT0/t5c8efZmqigqumVvPTe9tYeWCRq6YMYUKXfdFRMqIwp3sNWG+8fCLPPTsAVZdNp07b16ihVIRKWuxD/d9R3v4wg938Mqht/jD913Kl1Yt0ChdRMperMP9idc6ueX+nbg796y9ht+8bHqpSxIRKYpYhnt/v/Pdx9v5m/98nUXNk/n7T7+XSxpqS12WiEjRhO2h+ofA5wEHXgQ+B8wA7gcagB3Ap909FbLO0I73pPhFR7Zh9M93HWFPdw8fv2omf/Xx96ivqIhETsHhbmYzgS8DV7h7r5ltAj4BfBi4093vN7O7gHXA94pS7SikM/1sC8J8W3s3L/3qBO5QW13J0rn1fHn1Qj5+1Uy1shORSAo7LVMF1JhZHzAROAisAn4veHwj8CeUINzverKDb/3kdcZVGlfNnsZXVl/KygUNLJ5Vx7hKXXVBRKKt4HB39wNm9i3gTaAX+AnZaZjj7p4OnrYfmJnv9Wa2HlgPMHv27ELLOKtDb51ias04fnHbKiZWx3JpQURirOAhrJlNA24E5gIXA7XADSN9vbtvcPdWd29tamoqtIyz6kllmDyhSsEuIrEUZn7ifcAb7t7l7n3AQ8BKoM7MconaAhwIWWNBepIZJmqhVERiKky4vwksM7OJll2VXA28DDwO3BQ8Zy2wOVyJhUmk0hq1i0hsFRzu7r4deBB4luw2yApgA/B14Ktm1k52O+TdRahz1HpTGWrHa+QuIvEUamjr7t8Evjnk8G5gaZj3LYZEKkPdxOpSlyEiUhKR3RPYm0pr5C4isRXZcE+kMppzF5HYimy49yTT2i0jIrEVyXB3d3r6MtQq3EUkpiIZ7qf6+nGHGk3LiEhMRTLcE6ns1Q+0oCoicRXJcO9NZQC0oCoisRXJcM+N3LWgKiJxFclw7zk9cle4i0g8RTPck9lwrx2vaRkRiadIhntuWqZmnEbuIhJPkQz33IKqRu4iEleRDHctqIpI3EUy3HNz7gp3EYmraIa79rmLSMxFNNzTjK+qoLLCSl2KiEhJRDTcM1pMFZFYKzjczWyRme0c8OctM/uKmdWb2WNmtiv4Oa2YBY9Etn+q5ttFJL7C9FB9zd2XuPsS4L1AD/AwcCuwxd0XAluC+2OqJ5lRuItIrBVrWmY10OHue4EbgY3B8Y3AmiJ9xoj19KkLk4jEW7HC/RPAfcHtZnc/GNw+BDTne4GZrTezNjNr6+rqKlIZWerCJCJxFzrczawa+BjwL0Mfc3cHPN/r3H2Du7e6e2tTU1PYMgZR/1QRibtijNw/BDzr7oeD+4fNbAZA8LOzCJ8xKr2ptBp1iEisFSPcP8mZKRmAR4C1we21wOYifMaoZEfuCncRia9Q4W5mtcD7gYcGHL4deL+Z7QLeF9wfU72alhGRmAuVgO6eABqGHOsmu3umJNydRCpNrUbuIhJjkfuG6qm+ftyhRiN3EYmxyIV7T3C5Xy2oikicRTDcs1eEVBcmEYmzyIa7LhwmInEWuXBXFyYRkQiG+5kuTBq5i0h8RS/cNXIXEYliuGvOXUQkcuGuOXcRkQiGe+/p5tgKdxGJr8iFe0ILqiIi0Qv3nr4046sqqKywUpciIlIy0Qv3ZEaLqSISe5EL90QqrUsPiEjsRS7ce1MZXTRMRGIvcuGeSGV0uV8Rib3IhXtPUo06RETCttmrM7MHzexVM3vFzJabWb2ZPWZmu4Kf04pV7Ej0qMWeiEjokft3gB+7+2XAYuAV4FZgi7svBLYE98dMTyqtLzCJSOwVHO5mNhX4deBuAHdPuftx4EZgY/C0jcCasEWORo8WVEVEQo3c5wJdwA/M7Dkz+76Z1QLN7n4weM4hoDnfi81svZm1mVlbV1dXiDIG07SMiEi4cK8Crga+5+5XAQmGTMG4uwOe78XuvsHdW929tampKUQZg96ThKZlRERChft+YL+7bw/uP0g27A+b2QyA4GdnuBJHLpnux13XlRERKTjc3f0QsM/MFgWHVgMvA48Aa4Nja4HNoSochURSl/sVEYHs1EoYXwJ+ZGbVwG7gc2T/wdhkZuuAvcDNIT9jxHp0uV8RESBkuLv7TqA1z0Orw7xvodSFSUQkK1LfUM11YarRyF1EYi5S4Z7rwlSrBVURiblIhbsWVEVEsiIV7lpQFRHJimS4a0FVROIuYuGuBVUREYhYuCeSwbSM2uyJSMxFKtx7+tKMr6qgqjJSfy0RkVGLVAr2JDNaTBURIWrhrsv9iogAkQv3tBp1iIgQsXBPpDLUaOQuIhKtcO9NpanVnLuISLTCPaEFVRERIGLh3tunBVUREYhYuCeSWlAVEYGQzTrMbA/wNpAB0u7eamb1wAPAHGAPcLO7HwtX5sj0pDLUjNPIXUSkGCP333T3Je6e68h0K7DF3RcCW4L75527ayukiEjgfEzL3AhsDG5vBNach88YJpnup9/RnLuICOHD3YGfmNkOM1sfHGt294PB7UNAc8jPGBE16hAROSPsMPc6dz9gZtOBx8zs1YEPurubmed7YfCPwXqA2bNnhyxDjTpERAYKNXJ39wPBz07gYWApcNjMZgAEPzvP8toN7t7q7q1NTU1hygAGhrumZURECg53M6s1s8m528AHgJeAR4C1wdPWApvDFjkSuUYdE7WgKiISalqmGXjYzHLv88/u/mMzewbYZGbrgL3AzeHLfGenW+xp5C4iUni4u/tuYHGe493A6jBFFUILqiIiZ0TmG6q9fVpQFRHJiUy45/qn1o7XtIyISGTCPbegWqORu4hIlMI9mJYZp3AXEYlMuCdSaaqrKqiqjMxfSUSkYJFJwt5URl2YREQCkQn3bBcmLaaKiECEwr0nldY2SBGRQITCPcNEbYMUEQEiFe5pzbmLiAQiE+7ZOXeFu4gIRCjce/u0oCoikhOZcE8ktaAqIpITmXDvTWnkLiKSE4lwd3cSqTS1atQhIgJEJNyT6X76XRcNExHJiUS4qwuTiMhgocPdzCrN7DkzezS4P9fMtptZu5k9YGbV4cs8N3VhEhEZrBgj91uAVwbcvwO4090XAMeAdUX4jHM604VJI3cREQgZ7mbWAnwE+H5w34BVwIPBUzYCa8J8xkicHrlrQVVEBAg/cv828MdAf3C/ATju7ung/n5gZr4Xmtl6M2szs7aurq5QRahRh4jIYAWHu5l9FOh09x2FvN7dN7h7q7u3NjU1FVoGMGBBVRcOExEBIEwargQ+ZmYfBiYAU4DvAHVmVhWM3luAA+HLPLdc/1QtqIqIZBU8cnf329y9xd3nAJ8Afuruvw88DtwUPG0tsDl0le8gkdSCqojIQOdjn/vXga+aWTvZOfi7z8NnDHJ65K4FVRERINy0zGnu/gTwRHB7N7C0GO87UlpQFREZLDLfUK2uqqCqMhJ/HRGR0CKRhurCJCIyWCTCPduFSYupIiI5kQj33j416hARGSgS4Z5IZpioLzCJiJwWiXDvSaW1U0ZEZICIhHtGXZhERAaITLhrQVVE5IyIhLsWVEVEBopGuGsrpIjIIGUf7u5OQiN3EZFByj7ck+l++l0XDRMRGajsw/10ow5Ny4iInFb24Z7rn1qjaRkRkdPKPtx7+zRyFxEZquzDPTdy15y7iMgZZR/uvWrUISIyTMHhbmYTzOxpM3vezH5pZn8aHJ9rZtvNrN3MHjCz6uKVO1wit6CqC4eJiJwWZuSeBFa5+2JgCXCDmS0D7gDudPcFwDFgXfgyzy7XP1ULqiIiZxQc7p51Mrg7LvjjwCrgweD4RmBNqArfgbZCiogMF2rO3cwqzWwn0Ak8BnQAx909HTxlPzDzLK9db2ZtZtbW1dVVcA1aUBURGS5UuLt7xt2XAC3AUuCyUbx2g7u3untrU1NTwTVoQVVEZLii7JZx9+PA48ByoM7McnMkLcCBYnzG2SRSGaqrKqiqLPuNPyIiRRNmt0yTmdUFt2uA9wOvkA35m4KnrQU2hy3yXHpSaWq1mCoiMkiYVcgZwEYzqyT7j8Qmd3/UzF4G7jezvwCeA+4uQp1npUYdIiLDFZyK7v4CcFWe47vJzr+PCTXqEBEZruwnqhPJjMJdRGSIsg/3Xk3LiIgMU/bhnkilqdUedxGRQco+3HtTGWo0chcRGaTswz2hrZAiIsOUfbj3JDXnLiIyVFmHu7vT06fdMiIiQ5V1uCfT/WT6XRcNExEZoqzDvUcXDRMRyavMwz13uV/NuYuIDFTm4a5GHSIi+UQi3LWgKiIyWHmHe64Lk8JdRGSQsg73RG5aRnPuIiKDlHW45xZUazRyFxEZpMzDXQuqIiL5lHW4J5IauYuI5BOmh+osM3vczF42s1+a2S3B8Xoze8zMdgU/pxWv3MFm10/khnddpAVVEZEhzN0Le6HZDGCGuz9rZpOBHcAa4LPAUXe/3cxuBaa5+9fP9V6tra3e1tZWUB0iInFlZjvcvTXfYwWP3N39oLs/G9x+G3gFmAncCGwMnraRbOCLiMgYKsqcu5nNIdssezvQ7O4Hg4cOAc1nec16M2szs7aurq5ilCEiIoHQ4W5mk4B/Bb7i7m8NfMyzcz55533cfYO7t7p7a1NTU9gyRERkgFDhbmbjyAb7j9z9oeDw4WA+Pjcv3xmuRBERGa0wu2UMuBt4xd3/ZsBDjwBrg9trgc2FlyciIoUI8+2flcCngRfNbGdw7BvA7cAmM1sH7AVuDleiiIiMVsHh7u4/B+wsD68u9H1FRCS8sv6GqoiI5Ffwl5iKWoRZF9kpnEI0AkeKWE4U6Jzkp/MynM7JcOV0Ti5x97zbDS+IcA/DzNrO9g2tuNI5yU/nZTidk+Gick40LSMiEkEKdxGRCIpCuG8odQEXIJ2T/HRehtM5GS4S56Ts59xFRGS4KIzcRURkCIW7iEgElXW4m9kNZvaambUHjUFix8zuMbNOM3tpwLEx64Z1IboQuoRdaMxsgpk9bWbPB+fkT4Pjc81se/A79ICZVZe61rFmZpVm9pyZPRrcj8Q5KdtwN7NK4LvAh4ArgE+a2RWlraok7gVuGHLsVmCLuy8EtgT34yQNfM3drwCWAV8M/tuI83lJAqvcfTGwBLjBzJYBdwB3uvsC4BiwroQ1lsotZJsN5UTinJRtuANLgXZ33+3uKeB+sl2gYsXdfwYcHXI41t2w1CVsOM86GdwdF/xxYBXwYHA8VucEwMxagI8A3w/uGxE5J+Uc7jOBfQPu7w+OyQi7YcVBIV3CoiqYfthJtsfCY0AHcNzd08FT4vg79G3gj4H+4H4DETkn5RzuMgLn6oYVdYV2CYsqd8+4+xKghez/+V5W4pJKysw+CnS6+45S13I+hLmee6kdAGYNuN8SHJOgG5a7H4xrN6xzdQmL83kBcPfjZvY4sByoM7OqYKQat9+hlcDHzOzDwARgCvAdInJOynnk/gywMFjZrgY+QbYLlMS8G5a6hA1nZk1mVhfcrgHeT3Yt4nHgpuBpsTon7n6bu7e4+xyy+fFTd/99InJOyvobqsG/uN8GKoF73P0vS1zSmDOz+4DryV6m9DDwTeDfgE3AbIJuWO4+dNE1sszsOuC/gBc5M5f6DbLz7rE8L2Z2JdnFwUqyg7pN7v5nZjaP7GaEeuA54FPunixdpaVhZtcDf+TuH43KOSnrcBcRkfzKeVpGRETOQuEuIhJBCncRkQhSuIuIRJDCXUQkghTuIiIRpHAXEYmg/w+EhLpAN+t1BQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vuhj-HEciB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4473fe7b-c6bf-4f67-dde3-14bfda020845"
      },
      "source": [
        "plt.show()\n",
        "test = predict(model,X_test)\n",
        "test = pd.get_dummies(test)\n",
        "Y_test = pd.DataFrame(Y_test)\n",
        "print(\"Testing accuracy is: \",str(accuracy_score(Y_test, test) * 100)+\"%\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy is:  88.88888888888889%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}